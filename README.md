# LLM Local Experiments

This repository documents my experiments running large language models (LLMs) locally using an AMD RX 6700 XT GPU.

## ğŸ§  Purpose

Explore different ways of running LLMs locally with GPU acceleration ( ROCm ) on AMD hardware.

## ğŸ› ï¸ Setup

- **GPU**: AMD RX 6700 XT
- **OS**: [Windows 11/WSL2-UBUNTU 24.04]
- **Drivers**: [AMD driver version]
- **Backends tested**: ROCm

## Benchmarks


## ğŸ“ŒAMD GPU Compatibility Notes

The official ROCm support for RDNA2â€‘based GPUs (such as the RXâ€¯6700â€¯XT) remains limited.

To work around these limitations, I used the following community repositories:

- **likelovewant/ollama-for-amd**: Community fork adapting Ollama for broader ROCm support on RDNA2 GPUs like the RXâ€¯6700â€¯XT.  
  Repository: https://github.com/likelovewant/ollama-for-amd

- **ByronLeeeee/Ollamaâ€‘Forâ€‘AMDâ€‘Installer**: Automated installer and driverâ€patch tool for AMD GPUs to simplify ROCm setup on RDNA2 hardware.  
  Repository: https://github.com/ByronLeeeee/Ollama-For-AMD-Installer
